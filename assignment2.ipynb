{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcMf4aubeMI9"
   },
   "source": [
    "*****************************************************************\n",
    "#  The Social Web: data representation\n",
    "- Instructors: Jacco van Ossenbruggen.\n",
    "- TAs: Ayesha Noorain, Alex Boyko, Caio Silva, Elena Beretta, Mirthe Dankloff.\n",
    "- Exercises for Hands-on session 2\n",
    "*****************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhts5HMzeMI-"
   },
   "source": [
    "In this session you are going to mine data in various microformats. You will see the differences in what each of the formats can contain and what purpose they serve. We will start by looking at geographical data.\n",
    "\n",
    "Prerequisites:\n",
    "- Python 3.8\n",
    "- Python packages: requests, BeautifulSoup4, HTMLParser, rdflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "6f-OtFPPeMJA",
    "outputId": "9bcb836f-4204-4fac-d133-99e81a0b2884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hugoz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hugoz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hugoz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hugoz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hugoz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (1.26.12)\n",
      "Collecting BeautifulSoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "     -------------------------------------- 128.2/128.2 kB 7.4 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: soupsieve, BeautifulSoup4\n",
      "Successfully installed BeautifulSoup4-4.11.1 soupsieve-2.3.2.post1\n",
      "Collecting HTMLParser\n",
      "  Downloading HTMLParser-0.0.2.tar.gz (6.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Installing collected packages: HTMLParser\n",
      "  Running setup.py install for HTMLParser: started\n",
      "  Running setup.py install for HTMLParser: finished with status 'done'\n",
      "Successfully installed HTMLParser-0.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: HTMLParser is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdflib\n",
      "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
      "     ------------------------------------- 500.3/500.3 kB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\hugoz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rdflib) (58.1.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\hugoz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rdflib) (3.0.9)\n",
      "Collecting isodate\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 41.7/41.7 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in c:\\users\\hugoz\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from isodate->rdflib) (1.16.0)\n",
      "Installing collected packages: isodate, rdflib\n",
      "Successfully installed isodate-0.6.1 rdflib-6.2.0\n"
     ]
    }
   ],
   "source": [
    "# If you're using a virtualenv, make sure it's activated before running\n",
    "# this cell!\n",
    "!pip install requests\n",
    "!pip install BeautifulSoup4\n",
    "!pip install HTMLParser\n",
    "!pip install rdflib\n",
    "\n",
    "# IMPORTANT!!!\n",
    "# IN ORDER TO AVOID ERROR: \"ERROR: Could not build wheels for cryptography which use PEP 517 and cannot be installed directly\" when installing the requests package\n",
    "# CHECK THE requirements.txt file afterwards install them via the following command: pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irPnmIK4eMJd"
   },
   "source": [
    "##  Exercise 1\n",
    "\n",
    "Even if web pages do not use microformat, interesting data can often be extracted from the HTML. You may use packages such as BeautifulSoup to extract arbitrary pieces of data from any HTML page.\n",
    "The example below shows how we can find the URL of first image in the infobox table of the wikipedia page on Amsterdam. Tip: compare the code below with HTML source code of the wikipedia page: the image url is in the \"src\" attribute of the \"img\" element of in the \"table\" element with class=\"infobox\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9gpHw90keMJf",
    "outputId": "7ae1fe64-8d85-4a47-cfdf-422284954d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//upload.wikimedia.org/wikipedia/commons/thumb/b/be/KeizersgrachtReguliersgrachtAmsterdam.jpg/270px-KeizersgrachtReguliersgrachtAmsterdam.jpg\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# This script requires you to add a url of a page with geotags to the commandline, e.g.\n",
    "# python geo.py 'http://en.wikipedia.org/wiki/Amsterdam'\n",
    "URL = 'https://en.wikipedia.org/wiki/Amsterdam'\n",
    "\n",
    "req = requests.get(URL, headers={'User-Agent' : \"Social Web Course Student\"})\n",
    "soup = BeautifulSoup(req.text)\n",
    "# print(req.text)\n",
    "image1 = soup.findAll('table', class_='infobox')[0].find('img')\n",
    "print(image1['src'])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting coordinates from a webpage and reformatting them in the geo microformat (based on Example 8-1 in Mining the Social Web). Note that wikipages may encode long/lat information in different ways. On of the ways used by the Amsterdam wikipedia page is in a span element that is not shown to the user: \n",
    "<span class=\"geo\">52.367; 4.900</span>\n",
    "This span element has a single child: len(geoTag == 1) and no further structure, we have to manually get the long/lat by splitting the string on the ';' semicolon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LtHtQT9PeMJl",
    "outputId": "8a7f7b52-cdb2-409f-b3f0-ee7adf60a9f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"geo\">52.367; 4.900</span>\n",
      "Location is at 52.367 4.900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "geoTag = soup.find(True, 'geo')\n",
    "print(geoTag)\n",
    "\n",
    "if geoTag and len(geoTag) > 1:\n",
    "        lat = geoTag.find(True, 'latitude').string\n",
    "        lon = geoTag.find(True, 'longitude').string\n",
    "        print ('Location is at'), lat, lon\n",
    "elif geoTag and len(geoTag) == 1:\n",
    "        (lat, lon) = geoTag.string.split(';')\n",
    "        (lat, lon) = (lat.strip(), lon.strip())\n",
    "        print (('Location is at'), lat, lon)\n",
    "else:\n",
    "        print ('Location not found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S_bXnjveMJp"
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Can you convert the output of Exercise 1 into KML? Here is the KML documentation: https://developers.google.com/kml/documentation/?csw=1 and here you can find a simple example of how it is used: https://renenyffenegger.ch/notes/tools/Google-Earth/kml/index\n",
    "\n",
    "Visualise the point in Google Maps using the following code example: https://developers.google.com/maps/documentation/javascript/examples/layer-kml-features\n",
    "You will have to create your own KML file for the custom map layer, and provide a URL to the KML file inside the JavaScript code, which means that you have to upload the file somewhere. You can use a service like http://pastebin.com/ to obtain a URL for your KML file —> paste the code there and request the RAW format URL; use this one in this Task1.\n",
    "\n",
    "Is KML a microformat, why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KML is a microformat, moreover is an XML-based format provided by Google. \n",
    "#It is a microformat because it serves as consistent and descriptive metadata about an element, \n",
    "#it is representing a certain type of data, in our case, geographic coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<kml xmlns=\"http://www.opengis.net/kml/2.2\">\n",
    "  <Placemark>\n",
    "    <name>VrijeUniversity</name>\n",
    "    <description>This is the location of Vrije University Amsterdam.</description>\n",
    "    <Point>\n",
    "      <coordinates>52.333730555888685, 4.8656233385210035</coordinates>\n",
    "    </Point>\n",
    "  </Placemark>\n",
    "</kml>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (945164507.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    const map = new google.maps.Map(document.getElementById(\"map\"), {\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "function initMap() {\n",
    "  const map = new google.maps.Map(document.getElementById(\"map\"), {\n",
    "    zoom: 12,\n",
    "    center: { lat: 52.33, lng: 4.86 },\n",
    "  });\n",
    "  const kmlLayer = new google.maps.KmlLayer({\n",
    "    url: \"https://raw.githubusercontent.com/bobobo199733/the-social-web/main/vrije_location.kml\",\n",
    "    suppressInfoWindows: true,\n",
    "    map: map,\n",
    "  });\n",
    "\n",
    "  kmlLayer.addListener(\"click\", (kmlEvent) => {\n",
    "    const text = kmlEvent.featureData.description;\n",
    "\n",
    "    showInContentWindow(text);\n",
    "  });\n",
    "\n",
    "  function showInContentWindow(text) {\n",
    "    const sidebar = document.getElementById(\"sidebar\");\n",
    "\n",
    "    sidebar.innerHTML = text;\n",
    "  }\n",
    "}\n",
    "window.initMap = initMap;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUnka7EyeMJp"
   },
   "source": [
    "## Exercise 2 \n",
    "In order to find information in the web we can use microformats such as [hRecipe](https://microformats.org/wiki/hrecipe) or Schema.org's [Recipe](https://schema.org/Recipe). But first, we'll show you how to find arbitrary tags in a webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0pBs-PVeMJq"
   },
   "source": [
    "### Task 2 \n",
    "Parsing data for a <sub><sup>veggie</sup></sub> spaghetti alla carbonara recipe (from Example 2-7 in Mining the Social Web)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mt9BK_CZeMJr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# A yummy webpage (feel free to change to your likings.)\n",
    "URL = \"https://www.acouplecooks.com/spring-vegetarian-spaghetti-carbonara/\"\n",
    "\n",
    "# requests will return the html found at the given webpage...\n",
    "page = requests.get(URL)\n",
    "# ...and a BeautifulSoup object can be created from its content.\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "listchildren = list(soup.children)\n",
    "# print(listchildren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhdMwqykeMJt"
   },
   "source": [
    "We can find any element in the page through *css tag selectors*\n",
    "You can find them all [here](https://www.w3schools.com/cssref/css_selectors.asp), but shortly these are \".\" for classes, # for ids and plain text for the element name.\n",
    "\n",
    "\n",
    "You can also combine them, so that looking for \".class1.class2\" would select all elements displaying both classes. For a deeper overview please check the above link (or google \"html tag selectors\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "PBaiK8OLeMJu",
    "outputId": "5b75f973-41c1-4ad4-fd9f-4f1f7665ba1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[<li><span data-amount=\"1\">1</span> pound spaghetti noodles</li>, <li><span data-amount=\"0.5\" data-unit=\"cup\">½ cup</span> smoked mozzarella cheese</li>, <li><span data-amount=\"0.5\" data-unit=\"cup\">½ cup</span> grated Parmesan cheese, plus more for serving</li>, <li><span data-amount=\"4\">4</span> egg yolks</li>, <li><span data-amount=\"1\" data-unit=\"cup\">1 cup</span> frozen Earthbound Farm Organic peas</li>, <li><span data-amount=\"8\" data-unit=\"cup\">8 cups</span> Earthbound Farm Organic spinach</li>, <li><span data-amount=\"3\" data-unit=\"tablespoon\">3 tablespoons</span> butter</li>, <li><a class=\"tasty-link\" data-tasty-links-no-disclosure=\"\" href=\"https://www.acouplecooks.com/what-is-kosher-salt/\" rel=\"noopener\" target=\"_blank\">Kosher salt</a></li>, <li>Fresh ground black pepper</li>]\n"
     ]
    }
   ],
   "source": [
    "print(len(listchildren)) # we can see here how many children the html doc has got.\n",
    "ingredients_unparsed = soup.select_one(\".tasty-recipes-ingredients\")\n",
    "# let's get all the \"list item\" elements in a list:\n",
    "ing_unp = ingredients_unparsed.findAll('li')\n",
    "print(ing_unp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFXVPZhIeMJw"
   },
   "source": [
    "Mmmh... not so pretty yet. How about listing their items using the text method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "xASBZsnMeMJx",
    "outputId": "7af0f6e9-3b4f-4f34-e444-794087d06e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients:\n",
      "\n",
      "1 pound spaghetti noodles\n",
      "½ cup smoked mozzarella cheese\n",
      "½ cup grated Parmesan cheese, plus more for serving\n",
      "4 egg yolks\n",
      "1 cup frozen Earthbound Farm Organic peas\n",
      "8 cups Earthbound Farm Organic spinach\n",
      "3 tablespoons butter\n",
      "Kosher salt\n",
      "Fresh ground black pepper\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ingredients = [t.text for t in ing_unp]\n",
    "print(\"Ingredients:\\n\")\n",
    "# [print(i) for i in ingredients]  # Also prints the generator\n",
    "# Instead\n",
    "for ing in ingredients:\n",
    "    print(ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-RItVHyeMJz"
   },
   "source": [
    "Good. Now the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "d-3Op4B6eMJ0",
    "outputId": "75a70f0c-86d3-4be9-d2d8-84df91c4f392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li id=\"instruction-step-1\">In a large pot, combine 6 quarts of water with 2 tablespoons <a class=\"tasty-link\" data-tasty-links-no-disclosure=\"\" href=\"https://www.acouplecooks.com/what-is-kosher-salt/\" rel=\"noopener\" target=\"_blank\">kosher salt</a> and bring it to a boil.</li>, <li id=\"instruction-step-2\">Grate the Parmesan and mozzarella cheese. Carefully separate four egg yolks and set aside.</li>, <li id=\"instruction-step-3\">Once boiling, add the pasta and cook until the pasta is just about al dente, about 7 minutes; then add peas and spinach and cook for 1 minute. Reserve 1 cup cooking water, and then drain the pasta and vegetables.</li>, <li id=\"instruction-step-4\">In a skillet, melt the butter, then stir in the cheeses, ¼ cup pasta water, and ¼ teaspoon <a class=\"tasty-link\" data-tasty-links-no-disclosure=\"\" href=\"https://www.acouplecooks.com/what-is-kosher-salt/\" rel=\"noopener\" target=\"_blank\">kosher salt</a>. Stir in the pasta and vegetables until creamy over low heat, adding more pasta water if necessary (note that the mozzarella will stick together in some places).</li>, <li id=\"instruction-step-5\">To serve, top each pasta serving with a whole egg yolk and additional Parmesan cheese, and stir the yolk into the pasta at the table (if you are uncomfortable serving egg yolks at the table, stir the egg yolks into the pasta in the skillet to heat them through). Serve immediately. (Note that the mozzarella cheese can become gummy the longer the pasta sits, so eat immediately if possible. Leftovers can be reheated in a skillet, but may not have the same creamy texture.)</li>]\n"
     ]
    }
   ],
   "source": [
    "instructions_unparsed = soup.select_one(\".tasty-recipes-instructions\")\n",
    "instructions_unparsed = instructions_unparsed.findAll(\"li\")\n",
    "print(instructions_unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPWXuglfeMJ2"
   },
   "source": [
    "Let's finish off with the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yg1TnWe2eMJ3",
    "outputId": "05d39a2e-3779-45f1-ddeb-9c6d2ae5f494"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vegetarian Carbonara vegetarianJump to Recipeby Sonja OverhiserBuy Our Cookbook'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_unparsed = soup.select_one(\".post-header\") # \n",
    "categorical_title = title_unparsed.text.split(\"›\") # website specific divider.\n",
    "recipe_title = categorical_title[-1].strip() # let's remove that ugly space at the beginning.\n",
    "recipe_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYb6WtXYeMJ6"
   },
   "source": [
    "## Task 2.1\n",
    "Now it's your turn. Create a function that can scrape any recipe webpage from the same website (other websites will have different class tags). \n",
    "\n",
    "Make sure to:\n",
    "\n",
    "- return itemized content (e.g. ingredients) in a list. You may want to use a list comprehension here.\n",
    "- Not all items have been cleaned of their html markdown (see variables ```ingredients``` vs. ```instructions_unparsed```. Make sure to return a list with human readable content (i.e. by using the ```.text``` attribute).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "UQu9ecLEeMJ6",
    "outputId": "a8aa0e14-a8fb-4279-cf32-8dca97ab3412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 15-ounce cans green jackfruit in water or brine\n",
      "½ cup minced yellow onion\n",
      "3 cloves garlic\n",
      "2 tablespoons extra virgin olive oil\n",
      "1 to 1 1/2 tablespoons adobo sauce (from 1 can chipotle peppers in adobo)\n",
      "2 tablespoons cocoa powder\n",
      "2 tablespoons tomato paste\n",
      "2 teaspoons ground cumin\n",
      "1 teaspoon oregano\n",
      "¼ teaspoon kosher salt\n",
      "½ cup water\n",
      "8 tortillas (or more small street-style tortillas)\n",
      "Chopped romaine\n",
      "Fresh cilantro\n",
      "Salsa Fresca or any fresh salsa\n",
      "Refried beans, to serve on the side* (canned, Homemade Refried Beans, Instant Pot refried beans or Refried Black Beans)\n",
      "recipe:\n",
      "['Rinse and drain the jackfruit in a colander, pressing down to extract as much water as possible. Run your hands through the pieces, pulling and separating them into shreds with your fingers.', 'Mince the onion. Mince the garlic.', 'Heat the olive oil in a large skillet over medium heat. Add onion and garlic and saute for 3 to 4 minutes until tender and fragrant, but before the garlic browns. Add the jackfruit and remaining ingredients (add 1 tablespoon of adobo sauce for a mild recipe, up to 1 ½ or 2 tablespoons for a spicier recipe). Cook for about 5 minutes on medium low heat until saucy.', 'If desired, char the tortillas by placing them on an open gas flame on medium for a few seconds per side, flipping with tongs, until they are slightly blackened and warm. (See\\xa0How to Warm Tortillas.)', 'To serve, place the jackfruit, romaine, salsa fresca, and torn cilantro leaves in a warmed tortilla. Serve immediately with refried beans.']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pass in a URL containing hRecipe, such as\n",
    "# https://www.jamieoliver.com/recipes/pasta-recipes/veggie-carbonara/\n",
    "\n",
    "URL = \"https://www.acouplecooks.com/best-jackfruit-tacos/\"#YOUR RECIPE HERE/\n",
    "\n",
    "# Parse out some of the pertinent information for a recipe.\n",
    "# See http://microformats.org/wiki/hrecipe.\n",
    "\n",
    "def parse_website(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # You code here\n",
    "    # Parse header and get the title\n",
    "    title_unparsed = soup.select_one(\".post-header\") # \n",
    "    categorical_title = title_unparsed.text.split(\"›\") # website specific divider.\n",
    "    recipe_title = categorical_title[-1].strip() # let's remove that ugly space at the beginning.\n",
    "    fn = recipe_title\n",
    "\n",
    "    # Ingredients\n",
    "    ingredients_unparsed = soup.select_one(\".tasty-recipes-ingredients\")\n",
    "    # let's get all the \"list item\" elements in a list:\n",
    "    ing_unp = ingredients_unparsed.findAll('li')\n",
    "    ingredients = [t.text for t in ing_unp]\n",
    "\n",
    "    # Instructions\n",
    "    instructions_unparsed = soup.select_one(\".tasty-recipes-instructions\")\n",
    "    instructions_unparsed = instructions_unparsed.findAll(\"li\")\n",
    "    instructions = [t.text for t in instructions_unparsed]\n",
    "    for ing in ingredients:\n",
    "        print(ing)\n",
    "    return instructions\n",
    "    \n",
    "recipe = parse_website(URL)\n",
    "print(\"recipe:\")\n",
    "print (recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccURluAIeMJ8"
   },
   "source": [
    "But How can we get information not only from one website,  but from all? \n",
    "\n",
    "The answer: microformats.\n",
    "\n",
    "But rather than extracting with information manually from the schema.org or hRecipe microformats, we can use a package, ```scrape-schema-recipe``` \n",
    "\n",
    "Feel free to experiment with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBY-y_GreMJ8"
   },
   "source": [
    "### Task 2.2\n",
    "hRecipe is a microformat specifically created for recipes.\n",
    "Can you for example easily compare different dessert recipe ingredients? For inspiration you can look back at the exercises you did in Hands-on session 1 where you compared different sets of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both recipes contain:\n",
      "teaspoon kosher salt\n",
      "teaspoon allspice\n",
      "cup brown sugar\n",
      "\n",
      "Apple Dumplings (Grandma’s Recipe) vegan vegetarianJump to Recipeby Sonja OverhiserBuy Our Cookbook\n",
      "['2 cups all-purpose flour', '1 teaspoon kosher salt', '¼ teaspoon baking powder', '¾ cup unsalted butter*', '½ cup milk', '6 medium crisp tart apples\\xa0', '6 tablespoons granulated sugar\\xa0', '1 teaspoon cinnamon', '½ teaspoon allspice', '¼ teaspoon ginger', '3 tablespoons unsalted butter', '2/3 cup brown sugar', '⅓ cup water', '¼ teaspoon cinnamon', '2 tablespoons unsalted butter']\n",
      "Pumpkin Ice Cream vegetarianJump to Recipeby Sonja OverhiserBuy Our Cookbook\n",
      "['2 tablespoons cornstarch', '1 cup whole milk', '½ cup granulated sugar', '½ cup brown sugar', '2 teaspoons cinnamon', '1 teaspoon ground ginger', '½ teaspoon allspice', '¼ teaspoon nutmeg', '1 ½ cups heavy cream', '1 cup pumpkin puree', '1 tablespoon vanilla extract', '⅛ teaspoon kosher salt']\n"
     ]
    }
   ],
   "source": [
    "def fun(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # You code here\n",
    "    # Parse header and get the title\n",
    "    title_unparsed = soup.select_one(\".post-header\") # \n",
    "    categorical_title = title_unparsed.text.split(\"›\") # website specific divider.\n",
    "    recipe_title = categorical_title[-1].strip() # let's remove that ugly space at the beginning.\n",
    "    fn = recipe_title\n",
    "\n",
    "    # Ingredients\n",
    "    ingredients_unparsed = soup.select_one(\".tasty-recipes-ingredients\")\n",
    "    # let's get all the \"list item\" elements in a list:\n",
    "    ing_unp = ingredients_unparsed.findAll('li')\n",
    "    ingredients = [t.text for t in ing_unp]\n",
    "    return fn, ingredients\n",
    "\n",
    "title1, ing1 = fun(\"https://www.acouplecooks.com/apple-dumplings/\")\n",
    "title2, ing2 =fun(\"https://www.acouplecooks.com/pumpkin-ice-cream/\")\n",
    "\n",
    "print(\"both recipes contain:\")\n",
    "for _ in ing1:\n",
    "    for __ in ing2:\n",
    "        if _.split(\" \", 1)[1:] == __.split(\" \", 1)[1:]:\n",
    "            print(_.split(\" \", 1)[1])\n",
    "\n",
    "print()\n",
    "print(title1)\n",
    "print(ing1)\n",
    "\n",
    "print(title2)\n",
    "print(ing2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-J8fiLbeMJ9"
   },
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XBeqJHVeMJ9"
   },
   "source": [
    "Schema.org is one of the most widely used annotations formats. Schema.org is a multipurpose  template that has been created by a consortium consisting of Yahoo!, Google and Microsoft. It can describe entities, events, products etc. Check out the vocabulary specs on Schema.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiw8JClyeMJ-"
   },
   "source": [
    "### Task 3\n",
    "\n",
    "Parsing schema.org microdata. To parse this data you need to install the rdflib-microdata package, which you have done in one of the previous steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "X2zr3fOOeMJ-",
    "outputId": "d123f981-d73f-470f-b5e9-8735819f894b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/ontology/wikiPageWikiLink http://dbpedia.org/resource/Michael_Jackson\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://xmlns.com/foaf/0.1/isPrimaryTopicOf http://en.wikipedia.org/wiki/Micheal_Jackson\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/ontology/wikiPageRevisionID 1056738079\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://www.w3.org/2000/01/rdf-schema#label Micheal Jackson\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/property/wikiPageUsesTemplate http://dbpedia.org/resource/Template:R_from_misspelling\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/ontology/wikiPageLength 68\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/ontology/wikiPageID 14995602\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://www.w3.org/ns/prov#wasDerivedFrom http://en.wikipedia.org/wiki/Micheal_Jackson?oldid=1056738079&ns=0\n",
      "http://dbpedia.org/resource/Micheal_Jackson http://dbpedia.org/ontology/wikiPageRedirects http://dbpedia.org/resource/Michael_Jackson\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# Source: https://www.youtube.com/watch?v=sCU214rbRZ0\n",
    "# Pass in a URL containing Schema.org microformats\n",
    "URL = \"http://dbpedia.org/resource/Micheal_Jackson\"\n",
    "\n",
    "# Initialize a graph\n",
    "g = Graph()\n",
    "\n",
    "# Parse in an RDF file graph dbpedia\n",
    "result = g.parse(location=URL)\n",
    "\n",
    "# Loop through first 10 triples in the graph\n",
    "for index, (sub, pred, obj) in enumerate(g):\n",
    "    print(sub, pred, obj)\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hrQ2EuY5JAn1",
    "outputId": "eba60ebb-7ac5-4451-c16e-3f68e66af7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 9 facts\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the Graph\n",
    "print(f'Graph has {len(g)} facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "IAO1JllwJMqO",
    "outputId": "08f5e32d-d1a6-4a30-878a-ce7b768a8811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ns1: <http://dbpedia.org/ontology/> .\n",
      "@prefix ns2: <http://xmlns.com/foaf/0.1/> .\n",
      "@prefix ns3: <http://dbpedia.org/property/> .\n",
      "@prefix ns4: <http://www.w3.org/ns/prov#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "<http://dbpedia.org/resource/Micheal_Jackson> rdfs:label \"Micheal Jackson\"@en ;\n",
      "    ns1:wikiPageID 14995602 ;\n",
      "    ns1:wikiPageLength \"68\"^^xsd:nonNegativeInteger ;\n",
      "    ns1:wikiPageRedirects <http://dbpedia.org/resource/Michael_Jackson> ;\n",
      "    ns1:wikiPageRevisionID 1056738079 ;\n",
      "    ns1:wikiPageWikiLink <http://dbpedia.org/resource/Michael_Jackson> ;\n",
      "    ns3:wikiPageUsesTemplate <http://dbpedia.org/resource/Template:R_from_misspelling> ;\n",
      "    ns4:wasDerivedFrom <http://en.wikipedia.org/wiki/Micheal_Jackson?oldid=1056738079&ns=0> ;\n",
      "    ns2:isPrimaryTopicOf <http://en.wikipedia.org/wiki/Micheal_Jackson> .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the entire Graph in the RDF Turtle format\n",
    "print(g.serialize(format='ttl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzbynasAeMKA"
   },
   "source": [
    "### Task 3.1 \n",
    "Compare the schema.org information about a band on last.fm to the Facebook Open Graph information about the same band from Facebook. What are the differences? Which format do you think supports better interoperability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/ontology/wikiPageExternalLink https://books.google.com/books%3Fid=nQ1f7Vasrv8C\n",
      "http://dbpedia.org/resource/Pink_Floyd http://www.w3.org/2000/01/rdf-schema#label Pink Floyd\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/ontology/wikiPageWikiLink http://dbpedia.org/resource/The_Guardian\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/ontology/wikiPageWikiLink http://dbpedia.org/resource/Top_of_the_Pops\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/ontology/abstract Pink Floyd [pɪŋk flɔɪd] est un groupe britannique de rock originaire de Londres. Le groupe débute avec un premier album de musique psychédélique pour ensuite bifurquer vers le rock progressif. Formé en 1965, il est considéré comme un pionnier et représentant majeur de ces styles musicaux. Il est reconnu pour sa musique planante et expérimentale, ses textes philosophiques et satiriques, ses albums-concept et ses performances en concert originales et élaborées. De ses débuts à aujourd'hui, le groupe vend plus de 360 millions d'albums à travers le monde. Aux seuls États-Unis, les ventes des albums de Pink Floyd sont dénombrées par la RIAA à hauteur de 85 millions d'exemplaires. Initialement mené par le guitariste, chanteur et auteur-compositeur principal Syd Barrett, le groupe connaît un succès modeste au milieu des années 1960, puis devient l'un des groupes underground londoniens les plus populaires de la scène psychédélique. Cependant, le comportement de plus en plus instable de Barrett — principalement dû à son importante consommation de LSD — conduit les autres membres à le remplacer par David Gilmour, un ami d'enfance de Barrett. Le bassiste Roger Waters deviendra progressivement le meneur du groupe, signant toutes les paroles à partir de 1972. Pink Floyd acquiert l'année suivante une célébrité mondiale avec The Dark Side of the Moon (1973), troisième album le plus vendu de tous les temps, derrière Back in Black (1980) d'AC/DC et Thriller (1982) de Michael Jackson. Le groupe enchaîne les succès au cours des années 1970 avec Wish You Were Here (1975), Animals (1977) et The Wall (1979), ce dernier donnant lieu à une adaptation cinématographique. Néanmoins, des tensions dans le groupe apparaissent au fil du temps alors que Waters en prend de plus en plus le contrôle. Pendant l'enregistrement de l'album The Wall, le claviériste Richard Wright est renvoyé du groupe, mais participe tout de même à la tournée. Waters quitte le groupe en 1985, après la sortie deux ans plus tôt, de l'album The Final Cut dont il est l'unique auteur. En 1987, David Gilmour et Nick Mason décident d'enregistrer un nouvel album sous la bannière Pink Floyd et réintègrent Richard Wright pour A Momentary Lapse of Reason (1987) puis sur The Division Bell (1994) tout en recommençant à se produire en concert. Le groupe met ses activités en sommeil en 1996. Les différents membres se réunissent ensuite à quelques occasions dans les années 2000. Pink Floyd dans sa formation la plus connue (avec Gilmour, Mason, Waters et Wright) donne sa dernière prestation publique le 2 juillet 2005 lors du Live 8 à Londres. L'événement suscite de nombreuses rumeurs de reformation, démenties par Gilmour et devenues caduques avec la mort de Wright en 2008. Un ultime album portant le titre The Endless River, constitué principalement d'enregistrements non utilisés lors des séances de The Division Bell, est publié en novembre 2014.\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/property/wikiPageUsesTemplate http://dbpedia.org/resource/Template:Syd_Barrett\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/ontology/wikiPageWikiLink http://dbpedia.org/resource/The_Final_Cut_(album)\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/ontology/wikiPageExternalLink https://web.archive.org/web/20160624004447/https:/books.google.com/books%3Fid=DvgH58uEPFAC&pg=PA317%23v=onepage&q&f=false%7Carchive-date=24\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/property/wikt no\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/ontology/wikiPageWikiLink http://dbpedia.org/resource/Peter_Jenner\n",
      "http://dbpedia.org/resource/Pink_Floyd http://dbpedia.org/ontology/wikiPageWikiLink http://dbpedia.org/resource/Genesis_(band)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a graph\n",
    "g = Graph()\n",
    "URL_LAST = \"https://www.last.fm/music/Pink+Floyd\"\n",
    "URL_DB = \"http://dbpedia.org/resource/Pink_Floyd\"\n",
    "\n",
    "# Parse in an RDF file graph dbpedia\n",
    "result = g.parse(location=URL_DB)\n",
    "\n",
    "# Loop through first 10 triples in the graph\n",
    "for index, (sub, pred, obj) in enumerate(g):\n",
    "    print(sub, pred, obj)\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div data-require=\"tracking/tealium-utag-set\" data-tealium-data='{\"siteSection\": \"music\", \"pageType\": \"artist_door\", \"pageName\": \"music/artist/overview\", \"nativeEventTracking\": true, \"userState\": \"not authenticated\", \"userType\": \"anon\", \"musicArtistName\": \"Pink Floyd\", \"artist\": \"pinkfloyd\", \"ar\": \"pinkfloyd,davidgilmour,rogerwaters,sydbarrett,ledzeppelin\", \"tag\": \"progressiverock,psychedelicrock,classicrock,rock,psychedelic\"}' data-tealium-environment=\"prod\" id=\"initial-tealium-data\"></div>\n"
     ]
    }
   ],
   "source": [
    "page = requests.get(URL_LAST)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "data = soup.select_one(\".main-content\")\n",
    "print(soup.select_one(\"div\", {\"itemtype\": \"http://schema.org\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that dbpedia gives a more extensive result, that can be better interpreted, but for programming the one from last fm is easier to manipulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nocs4YDPeMKB"
   },
   "source": [
    "### Task 3.2\n",
    "Explore the various microformats at http://microformats.org/ and compare the output of the exercises with the output of http://microformats.org/. Think about possible microformats you want to support in your final assignment and read up on how to parse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/microformats/mf2py\n",
    "\n",
    "We will use a Json basesd microformat because they are universal and easy to manipulate in python. Storage of json object is also not a problem with non SQL databases"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hands-on_2_microformats.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a0138d27dc15ee4cb695af8dd977fbb152b4b260c442a2ca2d6adae8ed2f5ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
