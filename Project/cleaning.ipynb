{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "  tweets = re.sub('@[A-Za-z0–9]+', '', tweets) # remove tag(@)\n",
    "  tweets = re.sub('#[A-Za-z0–9]+', '', tweets) # remove hashtags(#)\n",
    "  tweets = re.sub('RT[\\s]+', '', tweets) # remove RT\n",
    "  tweets = re.sub('https?:\\/\\/\\S+', '', tweets) # remove URL links\n",
    "  tweets = re.sub('\\[(.*?)\\]', '', tweets) #remove fonts and writing styles, e.g. [$lt...$gt]\n",
    "  return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_shared(data_a, data_b):\n",
    "    clean_a, clean_b = [],[]\n",
    "    for a in data_a:\n",
    "        if a not in data_b:\n",
    "            clean_a.append(a)\n",
    "\n",
    "    for b in data_b:\n",
    "        if b not in data_a:\n",
    "            clean_b.append(b)\n",
    "\n",
    "    return pd.DataFrame(clean_a, columns=['Tweets']), pd.DataFrame(clean_b, columns=['Tweets'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    file = open(filename)\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_A = \"Raw\\data_before_Elon_Musk_DonaldTrump.json\"\n",
    "DATASET_B = \"Raw\\data_after_Elon_Musk_DonaldTrump.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_a = load_dataset(DATASET_A)\n",
    "data_b = load_dataset(DATASET_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_a, res_b = removing_shared(data_a, data_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_a.Tweets = res_a.Tweets.apply(clean_tweets)\n",
    "res_b.Tweets = res_b.Tweets.apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_a.to_csv(\"Clean\\clean_DT_before.csv\")\n",
    "res_b.to_csv(\"Clean\\clean_DT_after.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a0138d27dc15ee4cb695af8dd977fbb152b4b260c442a2ca2d6adae8ed2f5ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
