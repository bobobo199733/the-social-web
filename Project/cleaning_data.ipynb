{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook function to clean the duplicates between two datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Required Python libraries imports in order for this notebook to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import seaborn as sb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The following funtion remove all none text like objects from a tweet, in fact, it is a pure regex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "  tweets = re.sub('@(.*?)', '', tweets) # remove tag(@)\n",
    "  tweets = re.sub('#(.*?)', '', tweets) # remove hashtags(#)\n",
    "  tweets = re.sub('RT[\\s]+', '', tweets) # remove RT\n",
    "  tweets = re.sub('https?:\\/\\/\\S+', '', tweets) # remove URL links\n",
    "  tweets = re.sub('\\[(.*?)\\]', '', tweets) #remove fonts and writing styles, e.g. [$lt...$gt]\n",
    "  return tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The following function removes all shared tweets from the two input data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_shared(data_a, data_b):\n",
    "    clean_a, clean_b = [],[]\n",
    "    for a in data_a:\n",
    "        if a not in data_b:\n",
    "            clean_a.append(a)\n",
    "\n",
    "    for b in data_b:\n",
    "        if b not in data_a:\n",
    "            clean_b.append(b)\n",
    "\n",
    "    return pd.DataFrame(clean_a, columns=['Tweets']), pd.DataFrame(clean_b, columns=['Tweets'])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    file = open(filename)\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Applies all functions and saves the results under the \"Raw\" folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1. Clean tweets data for before/after Elon Musk in case of Donald Trump's related tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_A = \"Raw/data_before_Elon_Musk_DonaldTrump.json\"\n",
    "DATASET_B = \"Raw/data_after_Elon_Musk_DonaldTrump.json\"\n",
    "\n",
    "data_a = load_dataset(DATASET_A)\n",
    "data_b = load_dataset(DATASET_B)\n",
    "\n",
    "res_a, res_b = removing_shared(data_a, data_b)\n",
    "\n",
    "res_a.Tweets = res_a.Tweets.apply(clean_tweets)\n",
    "res_b.Tweets = res_b.Tweets.apply(clean_tweets)\n",
    "\n",
    "res_a.to_csv(\"Clean/clean_before_Elon_Musk_DonaldTrump.csv\")\n",
    "res_b.to_csv(\"Clean/clean_after_Elon_Musk_DonaldTrump.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2. Clean tweets data for before/after Elon Musk in case of Elon Musk's related tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_A = \"Raw/data_before_Elon_Musk.json\"\n",
    "DATASET_B = \"Raw/data_after_Elon_Musk.json\"\n",
    "\n",
    "data_a = load_dataset(DATASET_A)\n",
    "data_b = load_dataset(DATASET_B)\n",
    "\n",
    "res_a, res_b = removing_shared(data_a, data_b)\n",
    "\n",
    "res_a.Tweets = res_a.Tweets.apply(clean_tweets)\n",
    "res_b.Tweets = res_b.Tweets.apply(clean_tweets)\n",
    "\n",
    "res_a.to_csv(\"Clean/clean_before_Elon_Musk.csv\")\n",
    "res_b.to_csv(\"Clean/clean_after_Elon_Musk.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a0138d27dc15ee4cb695af8dd977fbb152b4b260c442a2ca2d6adae8ed2f5ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
